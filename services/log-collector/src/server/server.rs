use crate::buffer_batcher::log_buffer_batcher::InMemoryBuffer;
use crate::parser::parser::NormalizedLog;
use crate::proto::collector::log_collector_server::LogCollector;
use crate::proto::common::{CollectResponse, RawLog};
use crate::shipper::shipper::Shipper;
use futures::StreamExt;
use std::pin::Pin;
use tokio::sync::mpsc;
use tokio_stream::wrappers::ReceiverStream;
use tonic::{Request, Response, Status, transport::Server};

#[derive(Debug, Default)]
pub struct LogCollectorService {
    parser: NormalizedLog,
    buffer_batcher: InMemoryBuffer,
    shipper: Shipper,
}

// Bind log collector application logic(LogCollectorService) to
// the gRPC service contract(LogCollector trait generated by Tonic
// from collector.proto).
//
// Translates the network requests and the Log Collector
// actual logic(modules).
#[tonic::async_trait]
impl log_collector_server::LogCollector for LogCollectorService {
    // The type of response stream the server will send back to the client calling it;
    //
    // Client sends a stream of incoming logs(RawLog messages).
    // The server sends the client back a stream of responses(CollectResponse messages).
    //
    // Basically how gRPC bi-directional streaming works in Log Collector server.
    type StreamLogStream =
        Pin<Box<dyn tokio_stream::Stream<Item = Result<CollectResponse, Status>> + Send>>;

    // Server side implementation of the RPC. Tonic calls this function automatically when
    // a client starts streaming logs to the Log Collector.
    //
    // 1. request: Request<tonic::Streaming<RawLog>> - this is how the log collector receives
    // incoming stream of RawLog messages.
    // 2. Result<Response<Self::StreamLogsStream>, Status> - this is what the Log Collector sends
    // back: a response that contains its outgoing stream.
    //
    // Basically, a client starts sending logs, and log collector starts replying as it processes them.
    async fn stream_log(
        &self,
        request: Request<tonic::Streaming<RawLog>>,
    ) -> Result<Response<Self::StreamLogStream>, Status> {
        // Extract the stream of RawLog messages from the request
        let inbound_log_stream = request.into_inner();

        // Create a channel to send CollectResponses back to the client calling the Log Collector
        let (tx, rx) = mpsc::channel(32);

        // Clone references to modules(actual log collector logic) used by the Log Collector since self is &self
        let log_parser = self.parser.clone();
        let log_buffer_batcher = self.buffer_batcher.clone();
        let log_shipper = self.shipper.clone();

        // Spawn a background task to handle the incoming stream of logs
        tokio::spawn(async move {
            while let Some(raw_log_message) = inbound_log_stream.next().await {
                match raw_log_message {
                    Ok(raw_log) => {
                        let line = raw_log.line;

                        // Parse into NormalizedLog format
                        match NormalizedLog::select_parser(&line).await {
                            Ok(parsed_log) => {
                                // Add NormalizedLog to InMemoryBuffer
                                // Incoming RawLogs sit in-memory, until either; configured batch_size or batch_timeout_ms has elapsed
                                if let Err(e) = log_buffer_batcher.push(parsed_log).await {
                                    eprintln!("InMemoryBuffer error: {e}");
                                    let _ = tx
                                        .send(Ok(CollectRespnse {
                                            status: format!("in memory buffer error: {e}"),
                                        }))
                                        .await;
                                    continue;
                                }

                                // Attempt to flush InMemoryBuffer NormalizedLogs to SQLite
                                // when either batch_size, batch_timeout_ms or both is hit
                                // If running with durability, in-memory logs are written to disk (SQLite database),
                                // then cleared from in-memory queue. If not running with durability RawLogs just remain
                                // in-memory (RAM).
                                //
                                // The logs are "safe", they're either persisted (SQLite) or buffered in-memory
                                if log_buffer_batcher.queue.len() >= log_buffer_batcher.batch_size
                                    || log_buffer_batcher.last_flush_at.elapsed().as_millis()
                                        >= log_buffer_batcher.batch_timeout_ms as u128
                                {
                                    match log_buffer_batcher.flush(parsed_log.clone()).await {
                                        Ok(Some(flushed_logs)) => {
                                            // pass flushed logs batch to shipper
                                            if let Err(e) =
                                                log_shipper.send(flushed_logs.clone()).await
                                            {
                                                eprintln!("Shipper error: {e}");
                                                let _ = tx
                                                    .send(Ok(CollectResponse {
                                                        status: format!("shipper error: {e}"),
                                                    }))
                                                    .await;
                                                continue;
                                            }

                                            // Send ACK back to client
                                            let _ = tx
                                                .send(Ok(CollectResponse {
                                                    status: format!(
                                                        "Flushed {} logs successfully and sent to Shipper",
                                                        flushed_logs.len()
                                                    ),
                                                }))
                                                .await;
                                        }
                                        Ok(None) => continue,

                                        Err(e) => {
                                            eprintln!("Buffer flush error: {e}");
                                            let _ = tx
                                                .send(Ok(CollectResponse {
                                                    status: format!("flush error: {e}"),
                                                }))
                                                .await;
                                            continue;
                                        }
                                    }
                                }
                            }
                            Err(err) => {
                                eprintln!("Failed to parse log: {}", err);
                                let _ = tx
                                    .send(Ok(CollectResponse {
                                        status: format!("error: {}", err),
                                    }))
                                    .await;
                            }
                        }
                    }
                    Err(e) => {
                        eprintln!("Error receiving log: {}", e);
                        let _ = tx
                            .send(Ok(CollectResponse {
                                status: format!("stream error: {}", e),
                            }))
                            .await;
                    }
                }
            }
        });
        // Convert the receiver into a stream response.
        let response_stream = ReceiverStream::new(rx);
        Ok(Response::new(
            Box::pin(response_stream) as Self::StreamLogStream
        ))
    }
}

// How are we handling configuration file loading, for each module's user configs?
